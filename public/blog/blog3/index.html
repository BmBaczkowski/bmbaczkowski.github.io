<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Latent mixture models: JAGS vs. Stan | Blazej M. Baczkowski</title>
<meta name="keywords" content="JAGS, STAN, MCMC, Gibbs, HMC">
<meta name="description" content="How to implement latent mixture models in JAGS vs. Stan?">
<meta name="author" content="Blazej M. Baczkowski">
<link rel="canonical" href="https://bmbaczkowski.github.io/blog/blog3/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e690afcd5c523330d5c8b4d746eb158361600a015e99518d4d246a6ccab0cc19.css" integrity="sha256-5pCvzVxSMzDVyLTXRusVg2FgCgFemVGNTSRqbMqwzBk=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bmbaczkowski.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://bmbaczkowski.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://bmbaczkowski.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://bmbaczkowski.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bmbaczkowski.github.io/blog/blog3/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Latent mixture models: JAGS vs. Stan" />
<meta property="og:description" content="How to implement latent mixture models in JAGS vs. Stan?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bmbaczkowski.github.io/blog/blog3/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2025-03-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-22T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Latent mixture models: JAGS vs. Stan"/>
<meta name="twitter:description" content="How to implement latent mixture models in JAGS vs. Stan?"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://bmbaczkowski.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Latent mixture models: JAGS vs. Stan",
      "item": "https://bmbaczkowski.github.io/blog/blog3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Latent mixture models: JAGS vs. Stan",
  "name": "Latent mixture models: JAGS vs. Stan",
  "description": "How to implement latent mixture models in JAGS vs. Stan?",
  "keywords": [
    "JAGS", "STAN", "MCMC", "Gibbs", "HMC"
  ],
  "articleBody": "Why Mixture Models? In cognitive modeling, assuming that all observed data originate from a single generative process is often an oversimplification ‚Äì and in many cases, implausible. A common example is the presence of contaminant trials, where a participant responds using a cognitive process different from the one the model aims to capture. Such trials can distort inferences about the cognitive process of interest.\nA powerful way to address this challenge is through latent-mixture models1. The key idea is to extend the basic generative model to incorporate an additional component, for example, one that accounts for the contaminant process. In this framework, each trial is assumed to be generated by one of two (or more) underlying processes: the primary cognitive process of interest or the contaminant process. Since the specific origin of each trial is unknown, the model adopts a mixture structure, probabilistically assigning trials to different latent sources.\nBy explicitly modeling multiple data-generating processes, latent-mixture models improve robustness and provide more accurate inferences.\nImplementing Mixture Models: JAGS vs. Stan Mixture models are powerful, but their implementation can be tricky‚Äîand the approach varies significantly depending on the probabilistic programming language you choose.\nTwo of the most popular options are JAGS (Just Another Gibbs Sampler) and Stan. While both can handle mixture models, they take fundamentally different approaches:\nJAGS relies on Gibbs sampling with data augmentation, introducing a discrete indicator variable $z$ to assign each observation to a mixture component. Stan uses Hamiltonian Monte Carlo (HMC), which does not support discrete parameters in sampling. Instead, it requires marginalizing out the indicator variable $z$, leading to a different implementation strategy. In this post, we‚Äôll compare how mixture models are implemented in JAGS and Stan using a simple Gaussian Mixture Model (GMM).\nBuckle up! üöÄ\nToy Example: Gaussian Mixture Model (GMM) A two-component Gaussian mixture model assumes that each data point ($y_i$) is drawn from one of two normal distributions, indicated by a latent variable $z_i$ that assigns it to a component:\n$$ y_i \\sim \\mathcal{N}(\\mu_1, \\sigma_1), \\quad \\text{if } z_i = 1 $$\n$$ y_i \\sim \\mathcal{N}(\\mu_2, \\sigma_2), \\quad \\text{if } z_i = 0 $$\nThe probability of a data point belonging to the first component is given by $\\lambda$, so\n$$ z_i \\sim \\text{Bernoulli}(\\lambda) $$\nOur goal is to estimate $\\lambda$, $\\mu_1$, $\\mu_2$, $\\sigma_1$, and $\\sigma_2$ from observed data.\nJAGS Implementation: Explicit Latent Variables In JAGS, we explicitly introduce the latent variable $z$, which assigns each observation to a component. The model structure follows:\nmodel { for (i in 1:N) { z[i] ~ dbern(lambda) # Latent class assignment z1[i] \u003c- z[i] + 1 # Convert {0,1} to {1,2} for indexing y[i] ~ dnorm(mu[z1[i]], sigma_inv[z1[i]]) } # Priors lambda ~ dbeta(2, 2) # Prior for mixing proportion for (j in 1:2) { mu[j] ~ dnorm(0, 0.01) # Weakly informative prior sigma[j] ~ dunif(0, 5) sigma_inv[j] \u003c- 1 / pow(sigma[j], 2) # Deviation in terms of precision } } ‚úÖ Advantages: Conceptually simple, direct modeling of the indicator variable, making the mixture structure straightforward. ‚ö†Ô∏è Challenges: Discrete sampling can be inefficient, and mixing may be slow.\nStan Implementation: Marginalizing Out $z$ Unlike JAGS, Stan does not allow discrete latent variables in its HMC sampler. Stan‚Äôs HMC sampler requires differentiable parameters, meaning discrete latent variables must be marginalized out. Instead of sampling $z$, we compute the total likelihood across both possible assignments.\n$$ \\log p(y_i) = \\log \\left( \\lambda \\cdot \\mathcal{N}(y_i \\mid \\mu_1, \\sigma_1) + (1 - \\lambda) \\cdot \\mathcal{N}(y_i \\mid \\mu_2, \\sigma_2) \\right) $$\nFor numerical stability, we use the log_sum_exp trick:\ndata { int",
  "wordCount" : "728",
  "inLanguage": "en",
  "datePublished": "2025-03-22T00:00:00Z",
  "dateModified": "2025-03-22T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Blazej M. Baczkowski"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bmbaczkowski.github.io/blog/blog3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Blazej M. Baczkowski",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bmbaczkowski.github.io/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bmbaczkowski.github.io/" accesskey="h" title="Blazej M. Baczkowski">
                <img src="https://bmbaczkowski.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Blazej M. Baczkowski</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bmbaczkowski.github.io/research/" title="Research">
                    <span>Research</span>
                </a>
            </li>
            <li>
                <a href="https://bmbaczkowski.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://bmbaczkowski.github.io/courses/" title="Teaching">
                    <span>Teaching</span>
                </a>
            </li>
            <li>
                <a href="https://bmbaczkowski.github.io/books/" title="Books">
                    <span>Books</span>
                </a>
            </li>
            <li>
                <a href="https://bmbaczkowski.github.io/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://bmbaczkowski.github.io/consulting/" title="Consulting">
                    <span>Consulting</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Latent mixture models: JAGS vs. Stan
    </h1>
    <div class="post-meta"><span title='2025-03-22 00:00:00 +0000 UTC'>March 2025</span>&nbsp;&middot;&nbsp;Blazej M. Baczkowski

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#why-mixture-models"><strong>Why Mixture Models?</strong></a></li>
    <li><a href="#implementing-mixture-models-jags-vs-stan"><strong>Implementing Mixture Models: JAGS vs. Stan</strong></a></li>
    <li><a href="#toy-example-gaussian-mixture-model-gmm"><strong>Toy Example: Gaussian Mixture Model (GMM)</strong></a></li>
    <li><a href="#jags-implementation-explicit-latent-variables"><strong>JAGS Implementation: Explicit Latent Variables</strong></a></li>
    <li><a href="#stan-implementation-marginalizing-out-z"><strong>Stan Implementation: Marginalizing Out $z$</strong></a></li>
    <li><a href="#final-takeaway"><strong>Final Takeaway</strong></a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="why-mixture-models"><strong>Why Mixture Models?</strong><a hidden class="anchor" aria-hidden="true" href="#why-mixture-models">#</a></h2>
<p>In cognitive modeling, assuming that all observed data originate from a single generative process is often an oversimplification &ndash; and in many cases, implausible. A common example is the presence of <em>contaminant trials</em>, where a participant responds using a cognitive process different from the one the model aims to capture. Such trials can distort inferences about the cognitive process of interest.</p>
<p>A powerful way to address this challenge is through <strong>latent-mixture models</strong><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The key idea is to extend the basic generative model to incorporate an additional component, for example, one that accounts for the contaminant process. In this framework, each trial is assumed to be generated by one of two (or more) underlying processes: the primary cognitive process of interest or the contaminant process. Since the specific origin of each trial is unknown, the model adopts a mixture structure, probabilistically assigning trials to different latent sources.</p>
<p>By explicitly modeling multiple data-generating processes, latent-mixture models improve robustness and provide more accurate inferences.</p>
<hr>
<h2 id="implementing-mixture-models-jags-vs-stan"><strong>Implementing Mixture Models: JAGS vs. Stan</strong><a hidden class="anchor" aria-hidden="true" href="#implementing-mixture-models-jags-vs-stan">#</a></h2>
<p>Mixture models are powerful, but their implementation can be tricky‚Äîand the approach varies significantly depending on the probabilistic programming language you choose.</p>
<p>Two of the most popular options are <strong><a href="https://mcmc-jags.sourceforge.io/" target="_blank">JAGS</a></strong> (Just Another Gibbs Sampler) and <strong><a href="https://mc-stan.org/" target="_blank">Stan</a></strong>. While both can handle mixture models, they take fundamentally different approaches:</p>
<ul>
<li><strong>JAGS</strong> relies on Gibbs sampling with data augmentation, introducing a discrete indicator variable $z$ to assign each observation to a mixture component.</li>
<li><strong>Stan</strong> uses Hamiltonian Monte Carlo (HMC), which does not support discrete parameters in sampling. Instead, it requires marginalizing out the indicator variable $z$, leading to a different implementation strategy.</li>
</ul>
<p>In this post, we‚Äôll compare how mixture models are implemented in JAGS and Stan using a simple <strong>Gaussian Mixture Model (GMM)</strong>.</p>
<p>Buckle up! üöÄ</p>
<hr>
<h2 id="toy-example-gaussian-mixture-model-gmm"><strong>Toy Example: Gaussian Mixture Model (GMM)</strong><a hidden class="anchor" aria-hidden="true" href="#toy-example-gaussian-mixture-model-gmm">#</a></h2>
<p>A <strong>two-component Gaussian mixture model</strong> assumes that each data point ($y_i$) is drawn from one of two normal distributions, indicated by a latent variable $z_i$ that assigns it to a component:</p>
<p>$$
y_i \sim \mathcal{N}(\mu_1, \sigma_1), \quad \text{if } z_i = 1
$$</p>
<p>$$
y_i \sim \mathcal{N}(\mu_2, \sigma_2), \quad \text{if } z_i = 0
$$</p>
<p>The probability of a data point belonging to the first component is given by $\lambda$, so</p>
<p>$$
z_i \sim \text{Bernoulli}(\lambda)
$$</p>
<p>Our goal is to estimate $\lambda$, $\mu_1$, $\mu_2$, $\sigma_1$, and $\sigma_2$ from observed data.</p>
<hr>
<h2 id="jags-implementation-explicit-latent-variables"><strong>JAGS Implementation: Explicit Latent Variables</strong><a hidden class="anchor" aria-hidden="true" href="#jags-implementation-explicit-latent-variables">#</a></h2>
<p>In JAGS, we explicitly introduce the latent variable $z$, which assigns each observation to a component. The model structure follows:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>model {
</span></span><span style="display:flex;"><span>  for (i in 1:N) {
</span></span><span style="display:flex;"><span>    z[i] ~ dbern(lambda)  # Latent class assignment
</span></span><span style="display:flex;"><span>    z1[i] &lt;- z[i] + 1  # Convert {0,1} to {1,2} for indexing
</span></span><span style="display:flex;"><span>    y[i] ~ dnorm(mu[z1[i]], sigma_inv[z1[i]]) 
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  # Priors
</span></span><span style="display:flex;"><span>  lambda ~ dbeta(2, 2)  # Prior for mixing proportion
</span></span><span style="display:flex;"><span>  for (j in 1:2) {
</span></span><span style="display:flex;"><span>    mu[j] ~ dnorm(0, 0.01)  # Weakly informative prior
</span></span><span style="display:flex;"><span>    sigma[j] ~ dunif(0, 5)
</span></span><span style="display:flex;"><span>    sigma_inv[j] &lt;- 1 / pow(sigma[j], 2)  # Deviation in terms of precision
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>‚úÖ Advantages: Conceptually simple, direct modeling of the indicator variable, making the mixture structure straightforward.  <br>
‚ö†Ô∏è Challenges: Discrete sampling can be inefficient, and mixing may be slow.</p>
<hr>
<h2 id="stan-implementation-marginalizing-out-z"><strong>Stan Implementation: Marginalizing Out $z$</strong><a hidden class="anchor" aria-hidden="true" href="#stan-implementation-marginalizing-out-z">#</a></h2>
<p>Unlike JAGS, <strong>Stan does not allow discrete latent variables in its HMC sampler</strong>.
Stan‚Äôs HMC sampler requires differentiable parameters, meaning discrete latent variables must be marginalized out. Instead of sampling $z$, we compute the total likelihood across both possible assignments.</p>
<p>$$
\log p(y_i) = \log \left( \lambda \cdot \mathcal{N}(y_i \mid \mu_1, \sigma_1) + (1 - \lambda) \cdot \mathcal{N}(y_i \mid \mu_2, \sigma_2) \right)
$$</p>
<p>For numerical stability, we use the <code>log_sum_exp</code> trick:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>data {
</span></span><span style="display:flex;"><span>  int&lt;lower=0&gt; N;      
</span></span><span style="display:flex;"><span>  real y[N];          
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>parameters {
</span></span><span style="display:flex;"><span>  real mu1;               
</span></span><span style="display:flex;"><span>  real mu2;               
</span></span><span style="display:flex;"><span>  real&lt;lower=0&gt; sigma1;    
</span></span><span style="display:flex;"><span>  real&lt;lower=0&gt; sigma2;    
</span></span><span style="display:flex;"><span>  real&lt;lower=0, upper=1&gt; lambda;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model {
</span></span><span style="display:flex;"><span>  vector[N] log_lik1;
</span></span><span style="display:flex;"><span>  vector[N] log_lik2;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  // Priors
</span></span><span style="display:flex;"><span>  mu1 ~ normal(0, 5);
</span></span><span style="display:flex;"><span>  mu2 ~ normal(0, 5);
</span></span><span style="display:flex;"><span>  sigma1 ~ normal(0, 2);
</span></span><span style="display:flex;"><span>  sigma2 ~ normal(0, 2);
</span></span><span style="display:flex;"><span>  lambda ~ beta(2, 2);
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  // Compute log-likelihoods
</span></span><span style="display:flex;"><span>  for (n in 1:N) {
</span></span><span style="display:flex;"><span>	target += log_sum_exp(
</span></span><span style="display:flex;"><span>		log(lambda) + normal_lpdf(y[n] | mu1, sigma1),
</span></span><span style="display:flex;"><span>		log(1 - lambda) + normal_lpdf(y[n] | mu2, sigma2)
</span></span><span style="display:flex;"><span>	)
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>‚úÖ Advantages: More efficient sampling, better mixing than Gibbs sampling.<br>
‚ö†Ô∏è Challenges: Requires careful formulation, trickier to interpret than explicit latent assignments.</p>
<hr>
<h2 id="final-takeaway"><strong>Final Takeaway</strong><a hidden class="anchor" aria-hidden="true" href="#final-takeaway">#</a></h2>
<p>üöÄ Stan‚Äôs marginalization approach is generally superior for efficiency, but JAGS provides a more intuitive representation when discrete assignments are of interest.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Lee, M. D., &amp; Wagenmakers, E.-J. (2013). Bayesian cognitive modeling: A practical course. Cambridge University Press.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://bmbaczkowski.github.io/tags/jags/">JAGS</a></li>
      <li><a href="https://bmbaczkowski.github.io/tags/stan/">STAN</a></li>
      <li><a href="https://bmbaczkowski.github.io/tags/mcmc/">MCMC</a></li>
      <li><a href="https://bmbaczkowski.github.io/tags/gibbs/">Gibbs</a></li>
      <li><a href="https://bmbaczkowski.github.io/tags/hmc/">HMC</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://bmbaczkowski.github.io/">Blazej M. Baczkowski</a></span> ¬∑     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
